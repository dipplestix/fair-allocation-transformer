# Configuration for FFTransformerResidual trained on n=30, m=60
# Large model with d_model=256 to match 10x20 model parameter count (~3.9M params)
#
# To train:
#   uv run python training/train_residual.py --config configs/residual_30_60_large.yaml

n: 30
m: 60
d_model: 256
num_heads: 8
num_output_layers: 2
num_encoder_layers: 3
dropout: 0.09918628095456948
pool_config_name: row_col
residual_scale_init: 0.34560344700183276
lr: 0.0001  # Lower LR for larger model stability
weight_decay: 0.003062230822706729
batch_size: 128  # Larger batch for stability
steps: 100000
initial_temperature: 0.9325787187470638
final_temperature: 0.09837512050934188
grad_clip_norm: 1
seed: 42
checkpoint_dir: checkpoints/residual_30_60_large
checkpoint_every: 5000
keep_checkpoints: 3
val_every: 1000
val_size: 1000
patience: 30
min_delta: 1.0e-05
wandb_project: fa-transformer-residual-30-60-large
run_name: residual_30_60_large_production
